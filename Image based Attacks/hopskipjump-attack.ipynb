{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings\n\n# Suppress ConvergenceWarning from MLPClassifier for cleaner output\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.neural_network\")\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"overflow encountered in exp\")\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"invalid value encountered in true_divide\")\n\n# --- Helper Functions for HopSkipJump ---\n\ndef _is_misclassified(model, x, original_label):\n    \"\"\"\n    Checks if an input x is misclassified by the model compared to the original_label.\n    Assumes model.predict returns a single class label for a single input.\n    \"\"\"\n    prediction = model.predict(x.reshape(1, -1))[0]\n    return prediction != original_label\n\ndef _find_boundary_point(model, original_input, random_noise, original_label):\n    \"\"\"\n    Finds an initial point on the decision boundary using binary search.\n    Starts from a random point far away, and moves towards the original_input.\n    \"\"\"\n    # Start with a point far from the original input that is misclassified\n    # We add large noise to ensure it's misclassified initially\n    x_adv_initial = np.clip(original_input + random_noise, 0, 1)\n\n    if not _is_misclassified(model, x_adv_initial, original_label):\n        # If the initial noisy point isn't misclassified, try more noise.\n        # This can sometimes happen if the noise isn't large enough\n        # or if the model's decision boundary is very complex.\n        print(\"Initial noisy point not misclassified. Trying larger noise.\")\n        x_adv_initial = np.clip(original_input + (np.random.rand(*original_input.shape) * 2 - 1) * 2.0, 0, 1)\n        if not _is_misclassified(model, x_adv_initial, original_label):\n            print(\"Even with larger noise, initial point not misclassified. Cannot start attack.\")\n            return None\n\n    # Binary search to find a point on the decision boundary\n    low = 0.0\n    high = 1.0\n    boundary_point = x_adv_initial # Current candidate for adversarial point\n\n    # Perform binary search for a few iterations to get close to the boundary\n    for _ in range(30): # Binary search steps\n        mid = (low + high) / 2.0\n        candidate = original_input + mid * (x_adv_initial - original_input)\n        candidate = np.clip(candidate, 0, 1) # Ensure valid pixel range\n\n        if _is_misclassified(model, candidate, original_label):\n            boundary_point = candidate # This point is misclassified, try moving closer to original\n            high = mid\n        else:\n            low = mid # This point is correctly classified, move further from original\n            \n    return boundary_point\n\ndef _estimate_normal(model, x_current, x_original, original_label, num_samples=100, delta=1e-4):\n    \"\"\"\n    Estimates the normal vector to the decision boundary at x_current.\n    This is done by averaging the normalized vectors from x_current to nearby misclassified points.\n    \"\"\"\n    normal_vector = np.zeros_like(x_current)\n    \n    for _ in range(num_samples):\n        # Generate random noise direction\n        random_direction = np.random.randn(*x_current.shape)\n        random_direction = random_direction / np.linalg.norm(random_direction) # Normalize to unit vector\n\n        # Perturb x_current in both directions\n        x_plus = np.clip(x_current + delta * random_direction, 0, 1)\n        x_minus = np.clip(x_current - delta * random_direction, 0, 1)\n\n        # Check if the perturbed points are misclassified\n        is_plus_misclassified = _is_misclassified(model, x_plus, original_label)\n        is_minus_misclassified = _is_misclassified(model, x_minus, original_label)\n\n        if is_plus_misclassified != is_minus_misclassified:\n            # If one is misclassified and the other is not, it implies crossing the boundary\n            if is_plus_misclassified:\n                normal_vector += random_direction # Vector from current to misclassified side\n            else:\n                normal_vector -= random_direction # Vector from current to misclassified side\n\n    if np.linalg.norm(normal_vector) == 0:\n        # If no boundary crossing was detected, fall back to simple direction\n        return x_current - x_original\n    else:\n        return normal_vector / np.linalg.norm(normal_vector) # Normalize\n\n# --- HopSkipJump Attack Function ---\ndef hopskipjump_attack(\n    model,\n    original_input,\n    original_label,\n    max_iterations=1000, # Total iterations for the attack\n    initial_delta_factor=0.1, # Initial perturbation step size for boundary search\n    gamma=0.01,           # Step size multiplier for moving along the boundary\n    num_normal_samples=100, # Number of samples for normal estimation\n    normal_delta=1e-4     # Delta for normal estimation\n):\n    \"\"\"\n    Implements the HopSkipJump Attack, a query-efficient black-box attack.\n\n    Args:\n        model (sklearn.base.BaseEstimator): The trained scikit-learn classification model to attack.\n                                            Must have a `predict` method.\n        original_input (np.array): The initial input data (1D numpy array, values in [0, 1]).\n        original_label (int): The true label of the original input.\n        max_iterations (int): Total number of outer attack iterations.\n        initial_delta_factor (float): Factor to determine the initial random noise magnitude.\n        gamma (float): Multiplier for the step size when moving along the estimated boundary normal.\n        num_normal_samples (int): Number of random queries for normal vector estimation.\n        normal_delta (float): Small perturbation size for normal estimation.\n\n    Returns:\n        np.array: The adversarial example (perturbed input) if successful, otherwise None.\n    \"\"\"\n    print(f\"Starting HopSkipJump Attack for {max_iterations} iterations...\")\n    \n    # Step 1: Find an initial adversarial example on the decision boundary\n    # Start with a random point that is far from original_input and misclassified\n    random_noise_initial = (np.random.rand(*original_input.shape) * 2 - 1) * initial_delta_factor\n    x_adv = _find_boundary_point(model, original_input, random_noise_initial, original_label)\n\n    if x_adv is None:\n        print(\"Failed to find an initial boundary point. Attack cannot proceed.\")\n        return None\n\n    best_x_adv = np.copy(x_adv)\n    min_dist_to_original = np.linalg.norm(x_adv - original_input)\n\n    print(f\"Initial adversarial point found. L2 distance to original: {min_dist_to_original:.4f}\")\n\n    for i in range(max_iterations):\n        # Step 2: Estimate the normal vector to the decision boundary at x_adv\n        # This vector points from the correctly classified side to the misclassified side.\n        normal = _estimate_normal(model, x_adv, original_input, original_label, num_normal_samples, normal_delta)\n        \n        # Step 3: Move x_adv towards the original_input along the estimated normal\n        # This is the \"Hop\" step. We want to reduce the distance to original while staying misclassified.\n        # Project original_input onto the line defined by x_adv and normal\n        direction_to_original = original_input - x_adv\n        projection_onto_normal = np.dot(direction_to_original, normal) * normal\n        \n        # New candidate point: move x_adv towards original_input\n        x_adv_candidate = x_adv + projection_onto_normal\n\n        # Step 4: Binary search (or \"Jump\" step) to find the closest point on the boundary\n        # between x_adv_candidate and x_original (if it crossed)\n        \n        # Binary search bounds\n        low = 0.0\n        high = 1.0 # The full step to original\n\n        if _is_misclassified(model, x_adv_candidate, original_label):\n             # If candidate is still misclassified, it means we didn't cross the boundary or we are on the misclassified side of the boundary.\n             # We want to find the point on the boundary between x_adv and x_adv_candidate.\n            found_candidate_in_range = False\n            for _ in range(30): # Binary search steps\n                mid = (low + high) / 2.0\n                current_candidate_on_line = x_adv + mid * (x_adv_candidate - x_adv)\n                current_candidate_on_line = np.clip(current_candidate_on_line, 0, 1)\n\n                if _is_misclassified(model, current_candidate_on_line, original_label):\n                    x_adv = current_candidate_on_line # Keep moving towards original\n                    high = mid\n                    found_candidate_in_range = True\n                else:\n                    low = mid\n            if not found_candidate_in_range:\n                # If after binary search we couldn't find a boundary point, it means\n                # the initial x_adv_candidate was already on the misclassified side.\n                # In this case, simply keep x_adv as it is.\n                pass # x_adv retains its value from the previous iteration or initial boundary point.\n        else:\n            # Candidate is correctly classified, means we crossed the boundary.\n            # Perform binary search between x_adv (misclassified) and x_adv_candidate (correctly classified)\n            low_bs = 0.0\n            high_bs = 1.0\n            for _ in range(30):\n                mid_bs = (low_bs + high_bs) / 2.0\n                temp_x = x_adv + mid_bs * (x_adv_candidate - x_adv)\n                temp_x = np.clip(temp_x, 0, 1)\n                if _is_misclassified(model, temp_x, original_label):\n                    low_bs = mid_bs\n                else:\n                    high_bs = mid_bs\n            # Update x_adv to the point near the boundary\n            x_adv = np.clip(x_adv + low_bs * (x_adv_candidate - x_adv), 0, 1)\n\n\n        # Apply small random noise and project onto boundary to avoid getting stuck\n        random_step = np.random.randn(*x_adv.shape)\n        random_step = random_step / np.linalg.norm(random_step)\n        \n        x_adv = np.clip(x_adv + gamma * random_step, 0, 1)\n        \n        # Final projection onto [0,1] range\n        x_adv = np.clip(x_adv, 0, 1)\n        \n        # Check current distance and prediction\n        current_dist = np.linalg.norm(x_adv - original_input)\n        if current_dist < min_dist_to_original and _is_misclassified(model, x_adv, original_label):\n            min_dist_to_original = current_dist\n            best_x_adv = np.copy(x_adv)\n\n        if (i + 1) % (max_iterations // 10) == 0 or i == 0:\n            current_pred = model.predict(x_adv.reshape(1, -1))[0]\n            print(f\"Iteration {i+1}/{max_iterations}: Current Pred: {current_pred}, L2 Dist: {current_dist:.4f}, Misclassified: {_is_misclassified(model, x_adv, original_label)}\")\n            \n    # --- Final Check and Return ---\n    if best_x_adv is not None and _is_misclassified(model, best_x_adv, original_label):\n        final_l2_dist = np.linalg.norm(best_x_adv - original_input)\n        final_linf_norm = np.max(np.abs(best_x_adv - original_input))\n        print(f\"\\nHopSkipJump Attack successful! Adversarial example found. Final L2 Dist: {final_l2_dist:.4f}, L-inf Dist: {final_linf_norm:.4f}\")\n        return best_x_adv\n    else:\n        print(f\"\\nHopSkipJump Attack finished, but a successful adversarial example was not found.\")\n        return None\n\n# --- Example Usage with an Actual Model (MLPClassifier on Digits Dataset) ---\nif __name__ == \"__main__\":\n    np.random.seed(42) # Set seed for reproducibility of model training and sample selection\n\n    print(\"--- Loading and Training MLPClassifier on Digits Dataset ---\")\n    digits = load_digits()\n    X, y = digits.data, digits.target\n    num_features = X.shape[1] # Number of features (64 for 8x8 images)\n\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n    # Train an MLPClassifier to act as our \"black-box\" model\n    mlp_model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=1, verbose=False)\n    mlp_model.fit(X_train, y_train)\n    print(f\"Model training complete. Test accuracy: {mlp_model.score(X_test, y_test):.4f}\\n\")\n\n    # Select a random sample from the test set to attack\n    # Find a sample that the model initially classifies correctly\n    original_sample = None\n    original_label = -1\n    for i in range(len(X_test)):\n        sample = X_test[i]\n        label = y_test[i]\n        if mlp_model.predict(sample.reshape(1, -1))[0] == label:\n            original_sample = sample\n            original_label = label\n            break\n    \n    if original_sample is None:\n        print(\"Could not find a correctly classified sample to attack. Exiting.\")\n        exit()\n\n    original_predicted_class = mlp_model.predict(original_sample.reshape(1, -1))[0]\n    original_probabilities = mlp_model.predict_proba(original_sample.reshape(1, -1))[0]\n    original_logits = np.log(np.clip(original_probabilities, 1e-10, 1))\n\n    print(\"--- Original Sample Details ---\")\n    print(f\"Original Label: {original_label}\")\n    print(f\"Model's Predicted Class: {original_predicted_class}\")\n    print(f\"Model's Logits: {original_logits.round(4)}\")\n    print(f\"Input Shape: {original_sample.shape}\\n\")\n\n    # Run the HopSkipJump attack\n    # HopSkipJump is computationally intensive due to many queries.\n    # Adjust parameters carefully.\n    adversarial_sample = hopskipjump_attack(\n        mlp_model,\n        original_sample,\n        original_label,\n        max_iterations=100,  # Max iterations for the attack\n        initial_delta_factor=0.5, # Factor for initial random noise to find a misclassified point\n        gamma=0.01,           # Step size multiplier when moving along boundary\n        num_normal_samples=50, # Number of random queries for normal estimation (more = better estimate, slower)\n        normal_delta=1e-4     # Delta for normal estimation queries\n    )\n\n    # --- Display results if an adversarial example was found ---\n    if adversarial_sample is not None:\n        adv_predicted_class = mlp_model.predict(adversarial_sample.reshape(1, -1))[0]\n        adv_probabilities = mlp_model.predict_proba(adversarial_sample.reshape(1, -1))[0]\n        adv_logits = np.log(np.clip(adv_probabilities, 1e-10, 1))\n        \n        perturbation = adversarial_sample - original_sample\n        l2_norm_perturbation = np.linalg.norm(perturbation)\n        linf_norm_perturbation = np.max(np.abs(perturbation))\n\n        print(\"\\n--- HopSkipJump Attack Results ---\")\n        print(f\"Adversarial Input (first 5 features): {adversarial_sample[:5].round(4)}...\")\n        print(f\"Adversarial Logits: {adv_logits.round(4)}\")\n        print(f\"Adversarial Predicted Class: {adv_predicted_class}\")\n        print(f\"Perturbation (L2 Norm): {l2_norm_perturbation:.6f}\")\n        print(f\"Perturbation (L-inf Norm): {linf_norm_perturbation:.6f}\")\n\n\n        if adv_predicted_class != original_label:\n            print(\"\\nHopSkipJump attack successful: The adversarial example is now misclassified!\")\n        else:\n            print(\"\\nHopSkipJump attack inconclusive: Adversarial example found, but still classified as original label.\")\n    else:\n        print(\"\\nHopSkipJump attack failed to generate a suitable adversarial example.\")\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-28T14:08:19.318514Z","iopub.execute_input":"2025-06-28T14:08:19.319031Z","iopub.status.idle":"2025-06-28T14:08:24.930106Z","shell.execute_reply.started":"2025-06-28T14:08:19.318995Z","shell.execute_reply":"2025-06-28T14:08:24.928810Z"}},"outputs":[{"name":"stdout","text":"--- Loading and Training MLPClassifier on Digits Dataset ---\nModel training complete. Test accuracy: 0.9833\n\n--- Original Sample Details ---\nOriginal Label: 6\nModel's Predicted Class: 6\nModel's Logits: [-11.9587 -17.4366 -21.7252 -23.0259 -12.2853 -13.7145  -0.     -20.7407\n -11.1009 -18.7912]\nInput Shape: (64,)\n\nStarting HopSkipJump Attack for 100 iterations...\nInitial noisy point not misclassified. Trying larger noise.\nInitial adversarial point found. L2 distance to original: 1.6366\nIteration 1/100: Current Pred: 6, L2 Dist: 1.6354, Misclassified: False\nIteration 10/100: Current Pred: 6, L2 Dist: 1.6354, Misclassified: False\nIteration 20/100: Current Pred: 6, L2 Dist: 1.6361, Misclassified: False\nIteration 30/100: Current Pred: 6, L2 Dist: 1.6296, Misclassified: False\nIteration 40/100: Current Pred: 6, L2 Dist: 1.6349, Misclassified: False\nIteration 50/100: Current Pred: 6, L2 Dist: 1.6300, Misclassified: False\nIteration 60/100: Current Pred: 6, L2 Dist: 1.6270, Misclassified: False\nIteration 70/100: Current Pred: 6, L2 Dist: 1.6292, Misclassified: False\nIteration 80/100: Current Pred: 6, L2 Dist: 1.6230, Misclassified: False\nIteration 90/100: Current Pred: 6, L2 Dist: 1.6152, Misclassified: False\nIteration 100/100: Current Pred: 6, L2 Dist: 1.5969, Misclassified: False\n\nHopSkipJump Attack successful! Adversarial example found. Final L2 Dist: 1.5993, L-inf Dist: 0.3427\n\n--- HopSkipJump Attack Results ---\nAdversarial Input (first 5 features): [0.004  0.04   0.0025 0.595  0.5208]...\nAdversarial Logits: [ -7.6557  -0.9012  -1.9997 -13.9587  -3.009   -7.4186  -0.9041  -7.2937\n  -6.0013  -9.4291]\nAdversarial Predicted Class: 1\nPerturbation (L2 Norm): 1.599343\nPerturbation (L-inf Norm): 0.342715\n\nHopSkipJump attack successful: The adversarial example is now misclassified!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}