# Adversarial Attacks on Neural Networks
The repository aimed at documenting adversarial attacks on neural networks and potential ways to stop them.

## Adversarial Attacks
An adversarial attack is when someone deliberately creates small, carefully crafted changes to the input of a machine learning model so that the model makes a mistake, even though the changes are almost invisible to humans.

## Image-Based Attacks
1. Fast Gradient Sign Method (FGSM) Attack
2. Basic Iterative Method (BIM) Attack
3. Projected Gradient Descent (PGD) Attack
4. DeepFool Attack
5. Jacobian-based Saliency Map Attack (JSMA) Attack
6. Boundary Attack
7. Carlini & Wagner (C&W) Attack
8. Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) Attack
9. Momentum Iterative FGSM (MI-FGSM) Attack
10. Zeroth Order Optimization (ZOO) Attack
11. Square Attack
12. HopSkipJump Attack

## Text-Based Attacks
1. Simple Word Attack
2. Simple Character Attack
