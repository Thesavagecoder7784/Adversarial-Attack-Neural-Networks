{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings\n\n# Suppress ConvergenceWarning from MLPClassifier for cleaner output\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.neural_network\")\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"overflow encountered in tanh\")\n\n# --- L-BFGS Attack Function ---\ndef lbfgs_attack(\n    model,\n    original_input,\n    target_class,\n    c_value=1.0,         # Controls the trade-off: higher C means prioritize misclassification more\n    kappa=0.0,           # Confidence parameter: target logit must be kappa higher than others\n    max_iterations=1000, # Maximum iterations for the optimizer\n):\n    \"\"\"\n    Implements a targeted adversarial attack using the L-BFGS optimization algorithm.\n\n    This attack minimizes an objective function that balances the perturbation size (L2 norm)\n    and a classification loss designed to achieve targeted misclassification.\n    The input is transformed using tanh to optimize in an unconstrained space.\n\n    Args:\n        model (sklearn.base.BaseEstimator): The trained scikit-learn classification model to attack.\n                                            Must have a `predict_proba` method.\n        original_input (np.array): The initial input data (e.g., an image, feature vector).\n                                   Assumed to be a 1D numpy array with values typically in [0, 1].\n        target_class (int): The specific class index the adversarial example should be classified as.\n        c_value (float): A positive constant that weights the classification loss part of the objective.\n                         A higher 'c_value' makes the attack more aggressive in achieving\n                         misclassification, potentially at the cost of a larger perturbation.\n        kappa (float): A confidence parameter. The attack tries to make the target class's\n                       logit score be at least 'kappa' higher than the maximum logit of all\n                       other classes. Setting kappa > 0 makes the attack stronger.\n        max_iterations (int): The maximum number of optimization steps.\n\n    Returns:\n        np.array: The adversarial example (perturbed input) if successful, otherwise None.\n    \"\"\"\n    num_features = len(original_input)\n    \n    # Determine the number of classes from the model\n    try:\n        num_classes = len(model.classes_)\n    except AttributeError:\n        num_classes = model.predict_proba(original_input.reshape(1, -1)).shape[1]\n\n    # Transform original_input to 'w' space using arctanh, clipping to avoid domain errors\n    original_input_clamped = np.clip(original_input, 1e-6, 1 - 1e-6)\n    w_initial = np.arctanh(2 * original_input_clamped - 1)\n\n    # --- Define the Objective Function for L-BFGS ---\n    def objective_function(w):\n        \"\"\"\n        The objective function to be minimized.\n        Combines L2 perturbation distance and a misclassification term.\n        \"\"\"\n        # 1. Convert 'w' back to 'x' (input space [0, 1])\n        x = (np.tanh(w) + 1) / 2\n        x = np.clip(x, 0, 1) # Ensure values are within [0, 1]\n\n        # 2. Calculate the L2 distance squared\n        l2_dist_sq = np.sum((x - original_input)**2)\n\n        # 3. Get logits from the model\n        probabilities = model.predict_proba(x.reshape(1, -1))[0]\n        probabilities = np.clip(probabilities, 1e-10, 1) # Clip to avoid log(0)\n        logits = np.log(probabilities)\n\n        # 4. Calculate the classification loss term (similar to C&W's f_loss)\n        target_logit = logits[target_class]\n        other_logits = np.delete(logits, target_class)\n        max_other_logit = np.max(other_logits)\n        \n        # Loss term encourages target logit to be significantly higher than others\n        f_loss = np.maximum(max_other_logit - target_logit, -kappa)\n\n        # 5. Total objective value\n        total_loss = l2_dist_sq + c_value * f_loss\n        return total_loss\n\n    # --- Perform Optimization using L-BFGS-B ---\n    result = minimize(\n        fun=objective_function,  # The function to minimize\n        x0=w_initial,            # Initial guess for 'w'\n        method='L-BFGS-B',       # The L-BFGS-B optimization algorithm\n        options={'maxiter': max_iterations, 'disp': False} # Optimization options\n    )\n\n    # --- Process Results ---\n    if result.success:\n        optimized_w = result.x\n        adversarial_x = (np.tanh(optimized_w) + 1) / 2\n        adversarial_x = np.clip(adversarial_x, 0, 1) # Final clamping to [0, 1]\n\n        # Verify if the attack achieved the targeted misclassification\n        predicted_class_adv = model.predict(adversarial_x.reshape(1, -1))[0]\n        \n        if predicted_class_adv == target_class:\n            l2_dist = np.linalg.norm(adversarial_x - original_input)\n            print(f\"L-BFGS Attack successful! Adversarial example found with L2 distance: {l2_dist:.4f}\")\n            return adversarial_x\n        else:\n            print(f\"Optimization finished, but L-BFGS attack failed: Predicted class {predicted_class_adv} (expected {target_class}).\")\n            return None\n    else:\n        print(f\"L-BFGS Optimization failed: {result.message}\")\n        return None\n\n# --- Example Usage with an Actual Model (MLPClassifier on Digits Dataset) ---\nif __name__ == \"__main__\":\n    print(\"--- Loading and Training MLPClassifier on Digits Dataset ---\")\n    # 1. Load the Digits dataset (handwritten digits 0-9)\n    digits = load_digits()\n    X, y = digits.data, digits.target\n\n    # Digits data ranges from 0 to 16. Normalize to [0, 1] for attack compatibility.\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # 2. Split data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n    # 3. Train an MLPClassifier (a simple neural network)\n    mlp_model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=1, verbose=False)\n    mlp_model.fit(X_train, y_train)\n    print(f\"Model training complete. Test accuracy: {mlp_model.score(X_test, y_test):.4f}\\n\")\n\n    # 4. Select a random sample from the test set to attack\n    np.random.seed(0) # for reproducibility\n    sample_index = np.random.randint(0, len(X_test))\n    original_sample = X_test[sample_index]\n    original_label = y_test[sample_index]\n\n    # Get the model's initial prediction for the chosen sample\n    original_predicted_class = mlp_model.predict(original_sample.reshape(1, -1))[0]\n    original_probabilities = mlp_model.predict_proba(original_sample.reshape(1, -1))[0]\n    original_logits = np.log(np.clip(original_probabilities, 1e-10, 1)) # Calculate logits\n\n    print(\"--- Original Sample Details ---\")\n    print(f\"Original Label: {original_label}\")\n    print(f\"Model's Predicted Class: {original_predicted_class}\")\n    print(f\"Model's Logits: {original_logits.round(4)}\")\n    print(f\"Input Shape: {original_sample.shape} (8x8 image flattened to 64 features)\\n\")\n\n    # 5. Define the target class for the attack.\n    # We choose a target class different from the original predicted class.\n    # If the original prediction is correct, pick a random different class.\n    # If the original prediction is wrong, try to make it predict original_label + 1 (cyclic).\n    if original_predicted_class == original_label:\n        all_classes = np.arange(mlp_model.n_outputs_)\n        other_classes = all_classes[all_classes != original_label]\n        target_class_for_attack = np.random.choice(other_classes)\n        print(f\"Original prediction is CORRECT ({original_label}). Attempting L-BFGS attack to misclassify to Target Class: {target_class_for_attack}\\n\")\n    else:\n        target_class_for_attack = (original_predicted_class + 1) % mlp_model.n_outputs_\n        print(f\"Original prediction is INCORRECT ({original_predicted_class}). Attempting L-BFGS attack to misclassify to Target Class: {target_class_for_attack}\\n\")\n\n\n    # 6. Run the L-BFGS attack\n    # Adjust c_value and kappa to balance perturbation size vs. attack strength.\n    # These values might need tuning for different models and datasets.\n    adversarial_sample = lbfgs_attack(\n        mlp_model,\n        original_sample,\n        target_class_for_attack,\n        c_value=1.0,       # Initial c-value; try increasing if attack fails\n        kappa=0.0,         # A common starting point for kappa; increase for stronger confidence\n        max_iterations=500 # Number of iterations for the optimizer\n    )\n\n    # 7. Display results if an adversarial example was found\n    if adversarial_sample is not None:\n        adv_predicted_class = mlp_model.predict(adversarial_sample.reshape(1, -1))[0]\n        adv_probabilities = mlp_model.predict_proba(adversarial_sample.reshape(1, -1))[0]\n        adv_logits = np.log(np.clip(adv_probabilities, 1e-10, 1))\n        \n        perturbation = adversarial_sample - original_sample\n        l2_norm_perturbation = np.linalg.norm(perturbation)\n\n        print(\"\\n--- L-BFGS Attack Results ---\")\n        print(f\"Adversarial Input (first 5 features): {adversarial_sample[:5].round(4)}...\")\n        print(f\"Adversarial Logits: {adv_logits.round(4)}\")\n        print(f\"Adversarial Predicted Class: {adv_predicted_class}\")\n        print(f\"Perturbation (L2 Norm): {l2_norm_perturbation:.6f}\")\n\n        if adv_predicted_class == target_class_for_attack:\n            print(\"\\nAttack successful: The adversarial example is now classified as the target class!\")\n        else:\n            print(\"\\nAttack inconclusive: Adversarial example found, but not classified as the exact target class (or optimization couldn't achieve target).\")\n    else:\n        print(\"\\nL-BFGS attack failed to generate a suitable adversarial example.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T15:57:30.749606Z","iopub.execute_input":"2025-06-26T15:57:30.750435Z","iopub.status.idle":"2025-06-26T15:57:36.741130Z","shell.execute_reply.started":"2025-06-26T15:57:30.750400Z","shell.execute_reply":"2025-06-26T15:57:36.740067Z"}},"outputs":[{"name":"stdout","text":"--- Loading and Training MLPClassifier on Digits Dataset ---\nModel training complete. Test accuracy: 0.9833\n\n--- Original Sample Details ---\nOriginal Label: 4\nModel's Predicted Class: 4\nModel's Logits: [-21.7323 -19.4936 -23.0259 -23.0259  -0.     -23.0259 -17.5507 -16.9132\n -23.0259 -23.0259]\nInput Shape: (64,) (8x8 image flattened to 64 features)\n\nOriginal prediction is CORRECT (4). Attempting L-BFGS attack to misclassify to Target Class: 6\n\nL-BFGS Attack successful! Adversarial example found with L2 distance: 1.4428\n\n--- L-BFGS Attack Results ---\nAdversarial Input (first 5 features): [0.     0.     0.     0.8027 0.9286]...\nAdversarial Logits: [ -9.4788 -12.9278 -22.4388 -23.0259  -0.6932 -11.7264  -0.6932 -12.3716\n -13.6486 -23.0259]\nAdversarial Predicted Class: 6\nPerturbation (L2 Norm): 1.442760\n\nAttack successful: The adversarial example is now classified as the target class!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}