{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings\n\n# Suppress ConvergenceWarning from MLPClassifier for cleaner output\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.neural_network\")\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"overflow encountered in exp\") # Can occur with large logits in softmax\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"invalid value encountered in true_divide\") # Can occur with large logits in softmax\n\n# --- ZOO Attack Function ---\ndef zoo_attack(\n    model,\n    original_input,\n    target_class,\n    c_value=1.0,           # Controls the trade-off: higher C means prioritize misclassification more\n    kappa=0.0,             # Confidence parameter: target logit must be kappa higher than others\n    learning_rate=0.01,    # Step size for gradient descent\n    num_queries_per_dim=1, # Number of queries to estimate gradient for each dimension (often 1 or 2)\n    delta=1e-4,            # Small value for finite difference gradient approximation\n    total_iterations=1000, # Total iterations for the attack\n    epsilon_budget=0.1     # L-infinity norm budget for the perturbation\n):\n    \"\"\"\n    Implements a Zeroth-Order Optimization (ZOO) black-box attack.\n\n    This attack approximates gradients by querying the model, making it suitable\n    for scenarios where model internals (weights, gradients) are not accessible.\n\n    Args:\n        model (sklearn.base.BaseEstimator): The trained scikit-learn classification model to attack.\n                                            Must have a `predict_proba` method.\n        original_input (np.array): The initial input data (1D numpy array, values in [0, 1]).\n        target_class (int): The specific class index the adversarial example should be classified as.\n        c_value (float): Weight for the classification loss part of the objective.\n        kappa (float): Confidence parameter for targeted misclassification.\n        learning_rate (float): Step size for updating the adversarial example.\n        num_queries_per_dim (int): Number of queries to estimate gradient per dimension (typically 1 or 2).\n                                   Set to 1 for basic finite difference (symmetric for better accuracy).\n        delta (float): Small perturbation size for finite difference approximation.\n        total_iterations (int): Total number of outer attack iterations.\n        epsilon_budget (float): Maximum allowed L-infinity norm of the perturbation.\n\n    Returns:\n        np.array: The adversarial example (perturbed input) if successful, otherwise None.\n    \"\"\"\n    num_features = len(original_input)\n    \n    # Determine the number of classes from the model\n    try:\n        num_classes = len(model.classes_)\n    except AttributeError:\n        num_classes = model.predict_proba(original_input.reshape(1, -1)).shape[1]\n\n    # Initialize adversarial example as a copy of the original input\n    x_adv = np.copy(original_input)\n\n    # --- Objective Function for Gradient Estimation ---\n    def f_objective(current_x):\n        \"\"\"\n        Calculates the value of the objective function for ZOO.\n        This is similar to the C&W objective's classification loss part.\n        Args:\n            current_x (np.array): The current input being evaluated.\n        Returns:\n            float: The objective value.\n        \"\"\"\n        # Ensure x is within [0, 1] for model prediction\n        current_x_clipped = np.clip(current_x, 0, 1)\n        \n        # Get probabilities from the black-box model\n        probabilities = model.predict_proba(current_x_clipped.reshape(1, -1))[0]\n        # Logits can be approximated as log(probabilities). Clip to avoid log(0).\n        probabilities = np.clip(probabilities, 1e-10, 1)\n        logits = np.log(probabilities)\n\n        target_logit = logits[target_class]\n        other_logits = np.delete(logits, target_class)\n        max_other_logit = np.max(other_logits)\n\n        # We want to minimize this value; it becomes negative when target logit is high\n        return np.maximum(max_other_logit - target_logit, -kappa)\n\n    print(f\"Starting ZOO Attack for {total_iterations} iterations...\")\n    \n    # Keep track of the best adversarial example found so far\n    best_x_adv = None\n    best_f_objective = np.inf\n\n    for i in range(total_iterations):\n        # Estimate gradients for each feature\n        estimated_grad = np.zeros_like(original_input)\n        \n        for dim in range(num_features):\n            # Create perturbed inputs for finite difference approximation\n            x_plus_delta = np.copy(x_adv)\n            x_minus_delta = np.copy(x_adv)\n            \n            x_plus_delta[dim] += delta\n            x_minus_delta[dim] -= delta\n            \n            # Query the model for objective values\n            obj_plus = f_objective(x_plus_delta)\n            obj_minus = f_objective(x_minus_delta)\n            \n            # Finite difference gradient approximation\n            estimated_grad[dim] = (obj_plus - obj_minus) / (2 * delta)\n\n        # Update adversarial example using gradient descent\n        # We want to minimize the objective, so move in the negative gradient direction\n        x_adv -= learning_rate * estimated_grad\n        \n        # Project perturbation to stay within epsilon budget (L-infinity norm)\n        # 1. Calculate current perturbation relative to original input\n        perturbation = x_adv - original_input\n        # 2. Clip the perturbation to stay within [-epsilon_budget, epsilon_budget]\n        perturbation_clipped = np.clip(perturbation, -epsilon_budget, epsilon_budget)\n        # 3. Apply clipped perturbation to original input to get new x_adv\n        x_adv = original_input + perturbation_clipped\n\n        # Clip x_adv to stay within valid pixel range [0, 1]\n        x_adv = np.clip(x_adv, 0, 1)\n\n        # Check current status and update best adversarial example\n        current_pred = model.predict(x_adv.reshape(1, -1))[0]\n        current_f_obj_val = f_objective(x_adv) # Calculate objective value for current x_adv\n        \n        # If the current adversarial example achieves the target, check if it's \"better\" (e.g., smaller perturbation or better f_objective)\n        if current_pred == target_class:\n            if current_f_obj_val < best_f_objective: # We want f_objective to be as negative as possible\n                best_f_objective = current_f_obj_val\n                best_x_adv = np.copy(x_adv)\n\n        if (i + 1) % (total_iterations // 10) == 0 or i == 0:\n            current_l2_dist = np.linalg.norm(x_adv - original_input)\n            print(f\"Iteration {i+1}/{total_iterations}: Current Pred: {current_pred}, L2 Dist: {current_l2_dist:.4f}, f_obj: {current_f_obj_val:.4f}\")\n            if current_pred == target_class:\n                print(f\"  Target class {target_class} reached!\")\n                # We won't break early, but continue refining perturbation if a better f_obj is found\n\n    # --- Final Check and Return ---\n    if best_x_adv is not None:\n        final_pred_class = model.predict(best_x_adv.reshape(1, -1))[0]\n        if final_pred_class == target_class:\n            final_l2_dist = np.linalg.norm(best_x_adv - original_input)\n            print(f\"\\nZOO Attack successful! Adversarial example found. Final L2 Dist: {final_l2_dist:.4f}\")\n            return best_x_adv\n        else:\n            # Should ideally not happen if best_x_adv was truly classified as target_class\n            print(f\"\\nZOO Attack found a candidate, but final check predicts {final_pred_class} (expected {target_class}).\")\n            return None\n    else:\n        print(f\"\\nZOO Attack finished, but target class {target_class} not achieved within the iterations.\")\n        return None\n\n# --- Example Usage with an Actual Model (MLPClassifier on Digits Dataset) ---\nif __name__ == \"__main__\":\n    np.random.seed(42) # Set seed for reproducibility of model training and sample selection\n\n    print(\"--- Loading and Training MLPClassifier on Digits Dataset ---\")\n    digits = load_digits()\n    X, y = digits.data, digits.target\n\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n    # Train an MLPClassifier to act as our \"black-box\" model\n    mlp_model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=1, verbose=False)\n    mlp_model.fit(X_train, y_train)\n    print(f\"Model training complete. Test accuracy: {mlp_model.score(X_test, y_test):.4f}\\n\")\n\n    # Select a random sample from the test set to attack\n    sample_index = np.random.randint(0, len(X_test))\n    original_sample = X_test[sample_index]\n    original_label = y_test[sample_index]\n\n    original_predicted_class = mlp_model.predict(original_sample.reshape(1, -1))[0]\n    original_probabilities = mlp_model.predict_proba(original_sample.reshape(1, -1))[0]\n    original_logits = np.log(np.clip(original_probabilities, 1e-10, 1))\n\n    print(\"--- Original Sample Details ---\")\n    print(f\"Original Label: {original_label}\")\n    print(f\"Model's Predicted Class: {original_predicted_class}\")\n    print(f\"Model's Logits: {original_logits.round(4)}\")\n    print(f\"Input Shape: {original_sample.shape}\\n\")\n\n    # Define the target class for the attack.\n    # We choose a target class different from the original predicted class.\n    if original_predicted_class == original_label:\n        all_classes = np.arange(mlp_model.n_outputs_)\n        other_classes = all_classes[all_classes != original_label]\n        if len(other_classes) > 0:\n            target_class_for_attack = np.random.choice(other_classes)\n            print(f\"Original prediction is CORRECT ({original_label}). Attempting ZOO attack to misclassify to Target Class: {target_class_for_attack}\\n\")\n        else:\n            print(\"Only one class, cannot perform targeted attack.\")\n            exit()\n    else:\n        # If original prediction is already incorrect, pick a new target (e.g., original label + 1)\n        target_class_for_attack = (original_predicted_class + 1) % mlp_model.n_outputs_\n        print(f\"Original prediction is INCORRECT ({original_predicted_class}). Attempting ZOO attack to misclassify to Target Class: {target_class_for_attack}\\n\")\n\n\n    # --- Run the ZOO attack ---\n    adversarial_sample = zoo_attack(\n        mlp_model,\n        original_sample,\n        target_class_for_attack,\n        c_value=10.0,          # Increased for stronger classification push\n        kappa=0.0,             \n        learning_rate=0.01,    # Keeping this, can try slightly lower too\n        num_queries_per_dim=1, \n        delta=1e-4,            \n        total_iterations=5000, # Increased significantly\n        epsilon_budget=0.1     # Keeping this; can increase if still failing\n    )\n\n    # --- Display results if an adversarial example was found ---\n    if adversarial_sample is not None:\n        adv_predicted_class = mlp_model.predict(adversarial_sample.reshape(1, -1))[0]\n        adv_probabilities = mlp_model.predict_proba(adversarial_sample.reshape(1, -1))[0]\n        adv_logits = np.log(np.clip(adv_probabilities, 1e-10, 1))\n        \n        perturbation = adversarial_sample - original_sample\n        l2_norm_perturbation = np.linalg.norm(perturbation)\n        linf_norm_perturbation = np.max(np.abs(perturbation))\n\n        print(\"\\n--- ZOO Attack Results ---\")\n        print(f\"Adversarial Input (first 5 features): {adversarial_sample[:5].round(4)}...\")\n        print(f\"Adversarial Logits: {adv_logits.round(4)}\")\n        print(f\"Adversarial Predicted Class: {adv_predicted_class}\")\n        print(f\"Perturbation (L2 Norm): {l2_norm_perturbation:.6f}\")\n        print(f\"Perturbation (L-inf Norm): {linf_norm_perturbation:.6f}\")\n\n\n        if adv_predicted_class == target_class_for_attack:\n            print(\"\\nZOO attack successful: The adversarial example is now classified as the target class!\")\n        else:\n            print(\"\\nZOO attack inconclusive: Adversarial example found, but not classified as the exact target class.\")\n    else:\n        print(\"\\nZOO attack failed to generate a suitable adversarial example.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-28T13:51:38.434594Z","iopub.execute_input":"2025-06-28T13:51:38.434906Z","iopub.status.idle":"2025-06-28T13:53:04.589134Z","shell.execute_reply.started":"2025-06-28T13:51:38.434880Z","shell.execute_reply":"2025-06-28T13:53:04.587885Z"}},"outputs":[{"name":"stdout","text":"--- Loading and Training MLPClassifier on Digits Dataset ---\nModel training complete. Test accuracy: 0.9833\n\n--- Original Sample Details ---\nOriginal Label: 5\nModel's Predicted Class: 5\nModel's Logits: [-13.3782 -19.7226 -23.0259 -21.2603 -13.1114  -0.     -22.9556 -12.0312\n -22.2829 -10.7736]\nInput Shape: (64,)\n\nOriginal prediction is CORRECT (5). Attempting ZOO attack to misclassify to Target Class: 3\n\nStarting ZOO Attack for 5000 iterations...\nIteration 1/5000: Current Pred: 5, L2 Dist: 0.1751, f_obj: 17.6751\nIteration 500/5000: Current Pred: 5, L2 Dist: 0.6508, f_obj: 9.3848\nIteration 1000/5000: Current Pred: 5, L2 Dist: 0.6543, f_obj: 9.3846\nIteration 1500/5000: Current Pred: 5, L2 Dist: 0.6557, f_obj: 9.3845\nIteration 2000/5000: Current Pred: 5, L2 Dist: 0.6557, f_obj: 9.3845\nIteration 2500/5000: Current Pred: 5, L2 Dist: 0.6557, f_obj: 9.3845\nIteration 3000/5000: Current Pred: 5, L2 Dist: 0.6557, f_obj: 9.3845\nIteration 3500/5000: Current Pred: 5, L2 Dist: 0.6557, f_obj: 9.3845\nIteration 4000/5000: Current Pred: 5, L2 Dist: 0.6557, f_obj: 9.3845\nIteration 4500/5000: Current Pred: 5, L2 Dist: 0.6557, f_obj: 9.3845\nIteration 5000/5000: Current Pred: 5, L2 Dist: 0.6557, f_obj: 9.3845\n\nZOO Attack finished, but target class 3 not achieved within the iterations.\n\nZOO attack failed to generate a suitable adversarial example.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}